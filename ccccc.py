# fixed_evaluation.py
# ä¿®å¤matplotlibå…¼å®¹æ€§é—®é¢˜çš„è¯„ä¼°å·¥å…·

import torch
import numpy as np
import matplotlib

matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import warnings
import json
import os
from datetime import datetime

warnings.filterwarnings('ignore')

# å¯¼å…¥å¿…è¦æ¨¡å—
from dataset import FemurSegmentationDataset
from dataset_joint import JointSegmentationDataset
from dataset_tumor import TumorSegmentationDataset
from unet_smp import get_model

import config
import config_joint as cfg_joint
import config_tumor as cfg_tumor

from albumentations import Compose, Resize, Normalize
from albumentations.pytorch import ToTensorV2


class FixedModelEvaluator:
    """ä¿®å¤ç‰ˆæ¨¡å‹è¯„ä¼°å™¨ - è§£å†³matplotlibå…¼å®¹æ€§é—®é¢˜"""

    def __init__(self, device="cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # è®¾ç½®matplotlibæ ·å¼ - å…¼å®¹æ€§è®¾ç½®
        plt.style.use('default')
        plt.rcParams['font.size'] = 10
        plt.rcParams['font.weight'] = 'normal'
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['figure.dpi'] = 100
        plt.rcParams['savefig.dpi'] = 150
        plt.rcParams['savefig.bbox'] = 'tight'

        self.transform = Compose([
            Resize(512, 512),
            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
            ToTensorV2()
        ])

    def denormalize_image(self, img_tensor):
        """åå½’ä¸€åŒ–å›¾åƒç”¨äºæ˜¾ç¤º"""
        img_np = img_tensor.permute(1, 2, 0).cpu().numpy()
        img_np = (img_np + 1.0) / 2.0
        img_np = np.clip(img_np, 0, 1)
        return img_np

    def calculate_sample_metrics(self, pred_mask, gt_mask, num_classes):
        """è®¡ç®—å•ä¸ªæ ·æœ¬çš„æŒ‡æ ‡"""
        if isinstance(pred_mask, torch.Tensor):
            pred_mask = pred_mask.cpu().numpy()
        if isinstance(gt_mask, torch.Tensor):
            gt_mask = gt_mask.cpu().numpy()

        pred_flat = pred_mask.flatten()
        gt_flat = gt_mask.flatten()

        # æ•´ä½“å‡†ç¡®ç‡
        accuracy = (pred_flat == gt_flat).mean()

        # æ¯ç±»IoUå’ŒDice
        ious = []
        dices = []

        for class_id in range(num_classes):
            # è®¡ç®—TP, FP, FN
            tp = ((pred_flat == class_id) & (gt_flat == class_id)).sum()
            fp = ((pred_flat == class_id) & (gt_flat != class_id)).sum()
            fn = ((pred_flat != class_id) & (gt_flat == class_id)).sum()

            # IoUå’ŒDice
            iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 1.0
            dice = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 1.0

            ious.append(iou)
            dices.append(dice)

        # æ’é™¤èƒŒæ™¯ç±»åˆ«çš„mIoU
        if num_classes > 2:
            foreground_miou = np.mean(ious[1:])
            foreground_mdice = np.mean(dices[1:])
        else:
            foreground_miou = ious[1] if num_classes == 2 else np.mean(ious)
            foreground_mdice = dices[1] if num_classes == 2 else np.mean(dices)

        return {
            'accuracy': accuracy,
            'miou_all': np.mean(ious),
            'miou_foreground': foreground_miou,
            'mdice_all': np.mean(dices),
            'mdice_foreground': foreground_mdice,
            'per_class_iou': ious,
            'per_class_dice': dices
        }

    def evaluate_all_samples(self, model, dataset, num_classes, model_name):
        """è¯„ä¼°æ‰€æœ‰æ ·æœ¬å¹¶æ‰¾åˆ°æœ€ä½³ç»“æœ"""
        print(f"ğŸ” æ­£åœ¨è¯„ä¼° {model_name} çš„æ‰€æœ‰ {len(dataset)} ä¸ªæ ·æœ¬...")

        model.eval()
        all_results = []

        with torch.no_grad():
            for idx in tqdm(range(len(dataset)), desc=f"è¯„ä¼°{model_name}"):
                try:
                    img, gt_mask = dataset[idx]

                    # æ¨¡å‹æ¨ç†
                    img_batch = img.unsqueeze(0).to(self.device)
                    output = model(img_batch)
                    pred_probs = torch.softmax(output, dim=1)
                    pred_mask = output.argmax(dim=1).squeeze().cpu()

                    # è®¡ç®—æŒ‡æ ‡
                    metrics = self.calculate_sample_metrics(pred_mask, gt_mask, num_classes)

                    # ä¿å­˜ç»“æœ
                    result = {
                        'idx': idx,
                        'image': self.denormalize_image(img),
                        'gt_mask': gt_mask.cpu().numpy(),
                        'pred_mask': pred_mask.numpy(),
                        'pred_probs': pred_probs.squeeze().cpu().numpy(),
                        'metrics': metrics,
                        'score': metrics['miou_foreground'] * 0.6 + metrics['accuracy'] * 0.4
                    }

                    all_results.append(result)

                except Exception as e:
                    print(f"âš ï¸ æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                    continue

        print(f"âœ… æˆåŠŸè¯„ä¼° {len(all_results)} ä¸ªæ ·æœ¬")
        return all_results

    def get_best_samples(self, all_results, top_k=6):
        """è·å–æ•ˆæœæœ€å¥½çš„Kä¸ªæ ·æœ¬"""
        sorted_results = sorted(all_results, key=lambda x: x['score'], reverse=True)
        best_samples = sorted_results[:top_k]

        print(f"ğŸ† é€‰æ‹©äº†è¡¨ç°æœ€ä½³çš„ {len(best_samples)} ä¸ªæ ·æœ¬è¿›è¡Œå±•ç¤º")
        for i, sample in enumerate(best_samples):
            metrics = sample['metrics']
            print(
                f"  #{i + 1}: æ ·æœ¬{sample['idx']}, å‰æ™¯mIoU={metrics['miou_foreground']:.3f}, å‡†ç¡®ç‡={metrics['accuracy']:.3f}")

        return best_samples

    def calculate_overall_statistics(self, all_results):
        """è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
        if not all_results:
            return {}

        metrics_keys = ['accuracy', 'miou_all', 'miou_foreground', 'mdice_all', 'mdice_foreground']
        stats = {}

        for key in metrics_keys:
            values = [r['metrics'][key] for r in all_results]
            stats[key] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'median': np.median(values),
                'min': np.min(values),
                'max': np.max(values),
                'q25': np.percentile(values, 25),
                'q75': np.percentile(values, 75)
            }

        return stats

    def visualize_best_results(self, best_samples, class_names, stats, model_type):
        """å¯è§†åŒ–æœ€ä½³ç»“æœ - ä¿®å¤ç‰ˆæœ¬"""
        n_samples = len(best_samples)

        if model_type == 'multiclass':
            self._visualize_multiclass_fixed(best_samples, class_names, stats)
        elif model_type == 'tumor':
            self._visualize_tumor_fixed(best_samples, class_names, stats)
        elif model_type == 'joint':
            self._visualize_joint_fixed(best_samples, class_names, stats)

    def _visualize_multiclass_fixed(self, best_samples, class_names, stats):
        """å¤šç±»åˆ†å‰²å¯è§†åŒ– - ä¿®å¤ç‰ˆ"""
        n_samples = len(best_samples)

        # åˆ›å»ºé¢œè‰²æ˜ å°„
        colors = ['#000000', '#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F06292', '#AED581']
        cmap = ListedColormap(colors[:len(class_names)])

        fig, axes = plt.subplots(n_samples, 5, figsize=(20, 4 * n_samples))
        if n_samples == 1:
            axes = axes.reshape(1, -1)

        for i, sample in enumerate(best_samples):
            metrics = sample['metrics']

            # åŸå§‹å›¾åƒ
            axes[i, 0].imshow(sample['image'])
            axes[i, 0].set_title(f'æ ·æœ¬ {sample["idx"]}\nåŸå§‹Xå…‰ç‰‡', fontweight='bold')
            axes[i, 0].axis('off')

            # Ground Truth
            axes[i, 1].imshow(sample['gt_mask'], cmap=cmap, vmin=0, vmax=len(class_names) - 1)
            axes[i, 1].set_title('æ ‡å‡†ç­”æ¡ˆ', fontweight='bold')
            axes[i, 1].axis('off')

            # é¢„æµ‹ç»“æœ
            axes[i, 2].imshow(sample['pred_mask'], cmap=cmap, vmin=0, vmax=len(class_names) - 1)
            axes[i, 2].set_title(f'æ¨¡å‹é¢„æµ‹\nmIoU: {metrics["miou_foreground"]:.3f}', fontweight='bold')
            axes[i, 2].axis('off')

            # é¢„æµ‹ç½®ä¿¡åº¦
            max_prob = np.max(sample['pred_probs'], axis=0)
            im = axes[i, 3].imshow(max_prob, cmap='viridis', vmin=0, vmax=1)
            axes[i, 3].set_title(f'é¢„æµ‹ç½®ä¿¡åº¦\nå‡†ç¡®ç‡: {metrics["accuracy"]:.3f}', fontweight='bold')
            axes[i, 3].axis('off')

            # æŒ‡æ ‡å±•ç¤º
            axes[i, 4].axis('off')
            metrics_text = "å„ç±»åˆ«IoU:\n"
            for j, class_name in enumerate(class_names):
                iou = metrics['per_class_iou'][j]
                metrics_text += f"{class_name[:8]}: {iou:.3f}\n"

            axes[i, 4].text(0.05, 0.95, metrics_text, transform=axes[i, 4].transAxes,
                            fontsize=9, verticalalignment='top', fontfamily='monospace',
                            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8))

        plt.tight_layout()
        plt.suptitle(f'å¤šç±»éª¨éª¼åˆ†å‰²æœ€ä½³ç»“æœ\nå¹³å‡å‰æ™¯mIoU: {stats["miou_foreground"]["mean"]:.3f}',
                     fontsize=14, fontweight='bold', y=0.98)

        plt.savefig('fixed_multiclass_results.png', dpi=150, bbox_inches='tight')
        plt.close()  # å…³é—­å›¾å½¢é‡Šæ”¾å†…å­˜
        print("ğŸ’¾ ä¿å­˜: fixed_multiclass_results.png")

    def _visualize_tumor_fixed(self, best_samples, class_names, stats):
        """è‚¿ç˜¤åˆ†å‰²å¯è§†åŒ– - ä¿®å¤ç‰ˆ"""
        n_samples = len(best_samples)

        tumor_colors = [[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]
        tumor_cmap = ListedColormap(tumor_colors)

        fig, axes = plt.subplots(n_samples, 5, figsize=(20, 4 * n_samples))
        if n_samples == 1:
            axes = axes.reshape(1, -1)

        for i, sample in enumerate(best_samples):
            metrics = sample['metrics']

            # åŸå§‹å›¾åƒ
            axes[i, 0].imshow(sample['image'])
            axes[i, 0].set_title(f'æ ·æœ¬ {sample["idx"]}\nåŸå§‹Xå…‰ç‰‡', fontweight='bold')
            axes[i, 0].axis('off')

            # Ground Truth
            axes[i, 1].imshow(sample['gt_mask'], cmap=tumor_cmap, vmin=0, vmax=2)
            axes[i, 1].set_title('æ ‡å‡†ç­”æ¡ˆ\n(çº¢=è¡¨é¢, è“=éª¨å†…)', fontweight='bold')
            axes[i, 1].axis('off')

            # é¢„æµ‹ç»“æœ
            axes[i, 2].imshow(sample['pred_mask'], cmap=tumor_cmap, vmin=0, vmax=2)
            axes[i, 2].set_title(f'æ¨¡å‹é¢„æµ‹\nmIoU: {metrics["miou_foreground"]:.3f}', fontweight='bold')
            axes[i, 2].axis('off')

            # è¡¨é¢è‚¿ç˜¤æ¦‚ç‡
            if len(sample['pred_probs'].shape) == 3 and sample['pred_probs'].shape[0] > 1:
                surf_prob = sample['pred_probs'][1]
                axes[i, 3].imshow(surf_prob, cmap='Reds', vmin=0, vmax=1)
                axes[i, 3].set_title('è¡¨é¢è‚¿ç˜¤æ¦‚ç‡', fontweight='bold')
            else:
                axes[i, 3].text(0.5, 0.5, 'æ— æ¦‚ç‡æ•°æ®', ha='center', va='center', transform=axes[i, 3].transAxes)
                axes[i, 3].set_title('æ¦‚ç‡å›¾', fontweight='bold')
            axes[i, 3].axis('off')

            # æŒ‡æ ‡å±•ç¤º
            axes[i, 4].axis('off')
            metrics_text = f"""è‚¿ç˜¤åˆ†å‰²æŒ‡æ ‡:

å‰æ™¯mIoU: {metrics['miou_foreground']:.3f}
æ•´ä½“å‡†ç¡®ç‡: {metrics['accuracy']:.3f}
æ•´ä½“mIoU: {metrics['miou_all']:.3f}

å„ç±»åˆ«IoU:
èƒŒæ™¯: {metrics['per_class_iou'][0]:.3f}
è¡¨é¢: {metrics['per_class_iou'][1]:.3f}
éª¨å†…: {metrics['per_class_iou'][2]:.3f}
            """

            axes[i, 4].text(0.05, 0.95, metrics_text, transform=axes[i, 4].transAxes,
                            fontsize=9, verticalalignment='top', fontfamily='monospace',
                            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow", alpha=0.8))

        plt.tight_layout()
        plt.suptitle(f'è‚¿ç˜¤åˆ†å‰²æœ€ä½³ç»“æœ\nå¹³å‡å‰æ™¯mIoU: {stats["miou_foreground"]["mean"]:.3f}',
                     fontsize=14, fontweight='bold', y=0.98)

        plt.savefig('fixed_tumor_results.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("ğŸ’¾ ä¿å­˜: fixed_tumor_results.png")

    def _visualize_joint_fixed(self, best_samples, class_names, stats):
        """å…³èŠ‚åˆ†å‰²å¯è§†åŒ– - ä¿®å¤ç‰ˆ"""
        n_samples = len(best_samples)

        fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4 * n_samples))
        if n_samples == 1:
            axes = axes.reshape(1, -1)

        for i, sample in enumerate(best_samples):
            metrics = sample['metrics']

            # åŸå§‹å›¾åƒ
            axes[i, 0].imshow(sample['image'])
            axes[i, 0].set_title(f'æ ·æœ¬ {sample["idx"]}\nåŸå§‹Xå…‰ç‰‡', fontweight='bold')
            axes[i, 0].axis('off')

            # Ground Truth
            axes[i, 1].imshow(sample['gt_mask'], cmap='gray', vmin=0, vmax=1)
            axes[i, 1].set_title('æ ‡å‡†ç­”æ¡ˆ\n(ç™½=å…³èŠ‚)', fontweight='bold')
            axes[i, 1].axis('off')

            # é¢„æµ‹ç»“æœ
            axes[i, 2].imshow(sample['pred_mask'], cmap='gray', vmin=0, vmax=1)
            joint_iou = metrics['per_class_iou'][1] if len(metrics['per_class_iou']) > 1 else 0
            axes[i, 2].set_title(f'æ¨¡å‹é¢„æµ‹\nå…³èŠ‚IoU: {joint_iou:.3f}', fontweight='bold')
            axes[i, 2].axis('off')

            # æŒ‡æ ‡å±•ç¤º
            axes[i, 3].axis('off')
            metrics_text = f"""å…³èŠ‚åˆ†å‰²æŒ‡æ ‡:

å…³èŠ‚IoU: {joint_iou:.3f}
å…³èŠ‚Dice: {metrics['per_class_dice'][1] if len(metrics['per_class_dice']) > 1 else 0:.3f}
æ•´ä½“å‡†ç¡®ç‡: {metrics['accuracy']:.3f}
å‰æ™¯mIoU: {metrics['miou_foreground']:.3f}

è¯„åˆ†: {sample['score']:.3f}
            """

            axes[i, 3].text(0.05, 0.95, metrics_text, transform=axes[i, 3].transAxes,
                            fontsize=10, verticalalignment='top', fontfamily='monospace',
                            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.8))

        plt.tight_layout()
        plt.suptitle(f'å…³èŠ‚åˆ†å‰²æœ€ä½³ç»“æœ\nå¹³å‡å…³èŠ‚IoU: {stats["miou_foreground"]["mean"]:.3f}',
                     fontsize=14, fontweight='bold', y=0.98)

        plt.savefig('fixed_joint_results.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("ğŸ’¾ ä¿å­˜: fixed_joint_results.png")

    def create_fixed_performance_summary(self, all_model_results):
        """åˆ›å»ºä¿®å¤ç‰ˆæ€§èƒ½æ±‡æ€»å¯¹æ¯”"""
        if len(all_model_results) < 2:
            return

        print("\n" + "=" * 80)
        print("ğŸ æ‰€æœ‰æ¨¡å‹æ€§èƒ½æ±‡æ€»å¯¹æ¯”")
        print("=" * 80)

        # å‡†å¤‡å¯¹æ¯”æ•°æ®
        model_data = []
        for result in all_model_results:
            if result is None:
                continue
            stats = result['statistics']
            model_data.append({
                'model': result['model_type'].title(),
                'accuracy_mean': stats['accuracy']['mean'],
                'accuracy_std': stats['accuracy']['std'],
                'miou_mean': stats['miou_foreground']['mean'],
                'miou_std': stats['miou_foreground']['std'],
                'samples': len(result['all_results'])
            })

        # åˆ›å»ºç®€åŒ–çš„å¯¹æ¯”å›¾è¡¨ - é¿å…capthickå‚æ•°
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        models = [d['model'] for d in model_data]
        accuracies = [d['accuracy_mean'] for d in model_data]
        acc_stds = [d['accuracy_std'] for d in model_data]
        mious = [d['miou_mean'] for d in model_data]
        miou_stds = [d['miou_std'] for d in model_data]

        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(models)]

        # å‡†ç¡®ç‡å¯¹æ¯” - ä½¿ç”¨ç®€åŒ–çš„errorbar
        x_pos = np.arange(len(models))
        bars1 = ax1.bar(x_pos, accuracies, color=colors, alpha=0.8)
        ax1.errorbar(x_pos, accuracies, yerr=acc_stds, fmt='none', color='black', capsize=3)

        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_ylabel('å‡†ç¡®ç‡', fontsize=12)
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(models)
        ax1.set_ylim(0, 1)
        ax1.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (acc, std) in enumerate(zip(accuracies, acc_stds)):
            ax1.text(i, acc + std + 0.02, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

        # mIoUå¯¹æ¯”
        bars2 = ax2.bar(x_pos, mious, color=colors, alpha=0.8)
        ax2.errorbar(x_pos, mious, yerr=miou_stds, fmt='none', color='black', capsize=3)

        ax2.set_title('æ¨¡å‹å‰æ™¯mIoUå¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_ylabel('å‰æ™¯mIoU', fontsize=12)
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(models)
        ax2.set_ylim(0, 1)
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (miou, std) in enumerate(zip(mious, miou_stds)):
            ax2.text(i, miou + std + 0.02, f'{miou:.3f}', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()
        plt.suptitle('æ¨¡å‹æ€§èƒ½å…¨é¢å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold', y=1.02)
        plt.savefig('fixed_model_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("ğŸ’¾ ä¿å­˜: fixed_model_comparison.png")

        # æ‰“å°æ’å
        best_acc_idx = np.argmax(accuracies)
        best_miou_idx = np.argmax(mious)

        print(f"\nğŸ† æ€§èƒ½å† å†›:")
        print(f"   å‡†ç¡®ç‡æœ€é«˜: {models[best_acc_idx]} ({accuracies[best_acc_idx]:.3f})")
        print(f"   mIoUæœ€é«˜: {models[best_miou_idx]} ({mious[best_miou_idx]:.3f})")

    def run_fixed_evaluation(self):
        """è¿è¡Œä¿®å¤ç‰ˆè¯„ä¼°"""
        print("ğŸš€ å¯åŠ¨ä¿®å¤ç‰ˆæ¨¡å‹è¯„ä¼°ç³»ç»Ÿ")

        all_results = []

        # 1. å¤šç±»éª¨éª¼åˆ†å‰²
        try:
            print("\n" + "=" * 60)
            print("ğŸ¦´ Week 1: å¤šç±»éª¨éª¼åˆ†å‰²æ¨¡å‹è¯„ä¼°")
            print("=" * 60)

            dataset = FemurSegmentationDataset(transform=self.transform)
            model = get_model(config.NUM_CLASSES).to(self.device)
            checkpoint = torch.load("best_unet_smp.pth", map_location=self.device)
            model.load_state_dict(checkpoint)

            class_names = ['Background', 'Apophysis', 'Epiphysis', 'Metaphysis',
                           'Diaphysis', 'Surface Tumour', 'In-Bone Tumour', 'Joint']

            all_samples = self.evaluate_all_samples(model, dataset, config.NUM_CLASSES, "å¤šç±»éª¨éª¼åˆ†å‰²")
            best_samples = self.get_best_samples(all_samples)
            stats = self.calculate_overall_statistics(all_samples)

            self.visualize_best_results(best_samples, class_names, stats, 'multiclass')

            all_results.append({
                'model_type': 'multiclass',
                'all_results': all_samples,
                'best_samples': best_samples,
                'statistics': stats
            })

            print(f"\nğŸ“Š å¤šç±»éª¨éª¼åˆ†å‰²ç»Ÿè®¡:")
            print(f"   å‰æ™¯mIoU: {stats['miou_foreground']['mean']:.3f} Â± {stats['miou_foreground']['std']:.3f}")
            print(f"   å‡†ç¡®ç‡: {stats['accuracy']['mean']:.3f} Â± {stats['accuracy']['std']:.3f}")

        except Exception as e:
            print(f"âŒ å¤šç±»æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")

        # 2. è‚¿ç˜¤åˆ†å‰²
        try:
            print("\n" + "=" * 60)
            print("ğŸ¯ Week 2: è‚¿ç˜¤åˆ†å‰²æ¨¡å‹è¯„ä¼°")
            print("=" * 60)

            dataset = TumorSegmentationDataset(transform=self.transform, only_positive=False)
            model = get_model(cfg_tumor.NUM_CLASSES).to(self.device)
            checkpoint = torch.load(cfg_tumor.MODEL_NAME, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)

            class_names = ['Background', 'Surface Tumour', 'In-Bone Tumour']

            all_samples = self.evaluate_all_samples(model, dataset, cfg_tumor.NUM_CLASSES, "è‚¿ç˜¤åˆ†å‰²")
            best_samples = self.get_best_samples(all_samples)
            stats = self.calculate_overall_statistics(all_samples)

            self.visualize_best_results(best_samples, class_names, stats, 'tumor')

            all_results.append({
                'model_type': 'tumor',
                'all_results': all_samples,
                'best_samples': best_samples,
                'statistics': stats
            })

            print(f"\nğŸ“Š è‚¿ç˜¤åˆ†å‰²ç»Ÿè®¡:")
            print(f"   å‰æ™¯mIoU: {stats['miou_foreground']['mean']:.3f} Â± {stats['miou_foreground']['std']:.3f}")
            print(f"   å‡†ç¡®ç‡: {stats['accuracy']['mean']:.3f} Â± {stats['accuracy']['std']:.3f}")

        except Exception as e:
            print(f"âŒ è‚¿ç˜¤æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")

        # 3. å…³èŠ‚åˆ†å‰²
        try:
            print("\n" + "=" * 60)
            print("ğŸ”— Week 3: å…³èŠ‚åˆ†å‰²æ¨¡å‹è¯„ä¼°")
            print("=" * 60)

            dataset = JointSegmentationDataset(transform=self.transform, only_positive=False)
            model = get_model(cfg_joint.NUM_CLASSES).to(self.device)
            checkpoint = torch.load(cfg_joint.MODEL_NAME, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)

            class_names = ['Background', 'Joint']

            all_samples = self.evaluate_all_samples(model, dataset, cfg_joint.NUM_CLASSES, "å…³èŠ‚åˆ†å‰²")
            best_samples = self.get_best_samples(all_samples)
            stats = self.calculate_overall_statistics(all_samples)

            self.visualize_best_results(best_samples, class_names, stats, 'joint')

            all_results.append({
                'model_type': 'joint',
                'all_results': all_samples,
                'best_samples': best_samples,
                'statistics': stats
            })

            print(f"\nğŸ“Š å…³èŠ‚åˆ†å‰²ç»Ÿè®¡:")
            print(f"   å‰æ™¯mIoU: {stats['miou_foreground']['mean']:.3f} Â± {stats['miou_foreground']['std']:.3f}")
            print(f"   å‡†ç¡®ç‡: {stats['accuracy']['mean']:.3f} Â± {stats['accuracy']['std']:.3f}")

        except Exception as e:
            print(f"âŒ å…³èŠ‚æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")

        # åˆ›å»ºç»¼åˆå¯¹æ¯”
        if len(all_results) > 1:
            self.create_fixed_performance_summary(all_results)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        summary_data = {}
        for result in all_results:
            model_type = result['model_type']
            summary_data[model_type] = {
                'statistics': result['statistics'],
                'best_samples_indices': [s['idx'] for s in result['best_samples']],
                'total_samples': len(result['all_results'])
            }

        summary_data['evaluation_timestamp'] = datetime.now().isoformat()

        with open('fixed_evaluation_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… ä¿®å¤ç‰ˆè¯„ä¼°å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   ğŸ“Š fixed_multiclass_results.png - å¤šç±»åˆ†å‰²æœ€ä½³ç»“æœ")
        print(f"   ğŸ¯ fixed_tumor_results.png - è‚¿ç˜¤åˆ†å‰²æœ€ä½³ç»“æœ")
        print(f"   ğŸ”— fixed_joint_results.png - å…³èŠ‚åˆ†å‰²æœ€ä½³ç»“æœ")
        print(f"   ğŸ“ˆ fixed_model_comparison.png - æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print(f"   ğŸ“„ fixed_evaluation_summary.json - è¯¦ç»†ç»Ÿè®¡æ•°æ®")

        return all_results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä¿®å¤ç‰ˆæ¨¡å‹è¯„ä¼°å·¥å…·")
    print("è§£å†³matplotlibå…¼å®¹æ€§é—®é¢˜ï¼Œç”Ÿæˆæœ€ä½³é¢„æµ‹ç»“æœ")
    print()

    evaluator = FixedModelEvaluator()

    try:
        results = evaluator.run_fixed_evaluation()

        if results:
            print(f"\nğŸ‰ è¯„ä¼°æˆåŠŸå®Œæˆï¼")

            # æ‰“å°æœ€ç»ˆæ’å
            model_scores = []
            for result in results:
                stats = result['statistics']
                model_scores.append({
                    'name': result['model_type'].title(),
                    'miou': stats['miou_foreground']['mean'],
                    'accuracy': stats['accuracy']['mean']
                })

            # æŒ‰mIoUæ’åº
            model_scores.sort(key=lambda x: x['miou'], reverse=True)

            print(f"\nğŸ† æœ€ç»ˆæ¨¡å‹æ’å (æŒ‰å‰æ™¯mIoU):")
            for i, model in enumerate(model_scores):
                print(f"   {i + 1}. {model['name']:12} - mIoU: {model['miou']:.3f}, å‡†ç¡®ç‡: {model['accuracy']:.3f}")
        else:
            print(f"\nâš ï¸ æœªèƒ½æˆåŠŸè¯„ä¼°ä»»ä½•æ¨¡å‹")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()